{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2999de0",
   "metadata": {},
   "source": [
    "# ENSF 611 Final Project: Weather Temperature Prediction\n",
    "## Improving Temperature Forecasts Using Machine Learning\n",
    "\n",
    "**Group Members:** Cameron Dunn, Manuja Senanayake, Edmund Yu, Zohara Kamal\n",
    "\n",
    "### Project Overview\n",
    "This project aims to improve upon standard 24-hour advance temperature forecasts by analyzing the discrepancy between forecasted and observed temperatures in Calgary. We'll train and compare three regression models:\n",
    "- Linear Regression (baseline)\n",
    "- Support Vector Regressor (SVR)\n",
    "- Gradient Boosting Regressor\n",
    "\n",
    "### Objective\n",
    "Can a machine learning model find trends in the discrepancy between forecasted weather 24hrs in advance and observed temperature such that it can use the forecast to produce more accurate temperature predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b4580",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Machine Learning - Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e0f107",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forecast data (24-hour advance predictions)\n",
    "forecast_df = pd.read_csv('data/raw/forecast.csv', skiprows=2)\n",
    "\n",
    "# Load observed temperature data\n",
    "observed_df = pd.read_csv('data/raw/observed.csv', skiprows=2)\n",
    "\n",
    "print(f\"Forecast data shape: {forecast_df.shape}\")\n",
    "print(f\"Observed data shape: {observed_df.shape}\")\n",
    "print(\"\\nForecast data columns:\")\n",
    "print(forecast_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f56f0",
   "metadata": {},
   "source": [
    "## 3. Checking out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of forecast data\n",
    "print(\"Forecast Data:\")\n",
    "display(forecast_df.head())\n",
    "\n",
    "print(\"\\nObserved Data:\")\n",
    "display(observed_df.head())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nForecast Data Info:\")\n",
    "print(forecast_df.info())\n",
    "\n",
    "print(\"\\nObserved Data Info:\")\n",
    "print(observed_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c68a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing values in forecast data:\")\n",
    "print(forecast_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in observed data:\")\n",
    "print(observed_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fadb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Forecast Data Statistics:\")\n",
    "display(forecast_df.describe())\n",
    "\n",
    "print(\"\\nObserved Data Statistics:\")\n",
    "display(observed_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16815b7",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e27795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time column to datetime\n",
    "forecast_df['time'] = pd.to_datetime(forecast_df['time'])\n",
    "observed_df['time'] = pd.to_datetime(observed_df['time'])\n",
    "\n",
    "# Sort by time\n",
    "forecast_df = forecast_df.sort_values('time').reset_index(drop=True)\n",
    "observed_df = observed_df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "print(\"Date range of forecast data:\")\n",
    "print(f\"From: {forecast_df['time'].min()}\")\n",
    "print(f\"To: {forecast_df['time'].max()}\")\n",
    "\n",
    "print(\"\\nDate range of observed data:\")\n",
    "print(f\"From: {observed_df['time'].min()}\")\n",
    "print(f\"To: {observed_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge forecast and observed data on time\n",
    "df = forecast_df.merge(observed_df, on='time', how='inner', suffixes=('_forecast', '_observed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50225702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with more than 25% nulls\n",
    "null_threshold = 0.25\n",
    "cols_to_drop = df.columns[df.isnull().mean() > null_threshold]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(f\"Removed columns with >{null_threshold * 100}% nulls: {list(cols_to_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7411185",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f88538",
   "metadata": {},
   "source": [
    "### 5.1 Time Encoding (Hour, Day, Month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time components\n",
    "df_clean['hour'] = df_clean['time'].dt.hour\n",
    "df_clean['day'] = df_clean['time'].dt.day\n",
    "df_clean['month'] = df_clean['time'].dt.month\n",
    "df_clean['day_of_year'] = df_clean['time'].dt.dayofyear\n",
    "df_clean['day_of_week'] = df_clean['time'].dt.dayofweek\n",
    "\n",
    "# Cyclical encoding for hour (0-23)\n",
    "df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)\n",
    "df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)\n",
    "\n",
    "# Cyclical encoding for month (1-12)\n",
    "df_clean['month_sin'] = np.sin(2 * np.pi * df_clean['month'] / 12)\n",
    "df_clean['month_cos'] = np.cos(2 * np.pi * df_clean['month'] / 12)\n",
    "\n",
    "print(\"Time features created\")\n",
    "print(\"\\nNew time-based columns:\")\n",
    "print(['hour', 'day', 'month', 'day_of_year', 'day_of_week', \n",
    "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22480ee8",
   "metadata": {},
   "source": [
    "### 5.2 Wind Direction Encoding (Cosine and Sine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind direction is in degrees (0-360)\n",
    "# Convert to radians and apply sin/cos transformation\n",
    "wind_col = 'wind_direction_10m_previous_day1 (°)'\n",
    "\n",
    "if wind_col in df_clean.columns:\n",
    "    df_clean['wind_dir_sin'] = np.sin(np.radians(df_clean[wind_col]))\n",
    "    df_clean['wind_dir_cos'] = np.cos(np.radians(df_clean[wind_col]))\n",
    "else:\n",
    "    print(f\"Warning: {wind_col} not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1927c",
   "metadata": {},
   "source": [
    "## 6. Visualization and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81494031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure forecast_error exists for EDA/plots (not used in modeling!)\n",
    "df_clean['forecast_error'] = df_clean['temperature_2m_previous_day1 (°C)'] - df_clean['temperature_2m (°C)']\n",
    "\n",
    "# Visualize temperature trends\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot first 30 days\n",
    "sample_data = df_clean.head(24*30)  # First 30 days\n",
    "\n",
    "axes[0].plot(sample_data['time'], sample_data['temperature_2m_previous_day1 (°C)'], \n",
    "             label='Forecast', alpha=0.7, linewidth=1)\n",
    "axes[0].plot(sample_data['time'], sample_data['temperature_2m (°C)'], \n",
    "             label='Observed', alpha=0.7, linewidth=1)\n",
    "axes[0].set_title('Forecast vs Observed Temperature (First 30 Days)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Temperature (°C)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot forecast error\n",
    "axes[1].plot(sample_data['time'], sample_data['forecast_error'], \n",
    "             color='red', alpha=0.6, linewidth=1)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_title('Forecast Error (Forecast - Observed)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Error (°C)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of forecast error\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_clean['forecast_error'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df_clean['forecast_error'].mean(), color='red', \n",
    "                linestyle='--', linewidth=2, label=f\"Mean: {df_clean['forecast_error'].mean():.2f}°C\")\n",
    "axes[0].set_title('Distribution of Forecast Error', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Error (°C)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by month\n",
    "df_clean.boxplot(column='forecast_error', by='month', ax=axes[1])\n",
    "axes[1].set_title('Forecast Error by Month', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Error (°C)')\n",
    "plt.suptitle('')  # Remove the default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Absolute Forecast Error: {abs(df_clean['forecast_error']).mean():.2f}°C\")\n",
    "print(f\"RMSE of raw forecast: {np.sqrt(mean_squared_error(df_clean['temperature_2m (°C)'], df_clean['temperature_2m_previous_day1 (°C)'])):.2f}°C\")\n",
    "\n",
    "# Correlation heatmap\n",
    "# Select numeric columns for correlation\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "correlation_cols = [col for col in numeric_cols if col not in ['time']]\n",
    "\n",
    "# Calculate correlation with target variable\n",
    "target_corr = df_clean[correlation_cols].corrwith(df_clean['temperature_2m (°C)']).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "target_corr.plot(kind='barh')\n",
    "plt.title('Correlation with Observed Temperature', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features correlated with observed temperature:\")\n",
    "print(target_corr.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054ea38",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Modeling / Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the forecast as feature, and observed as target\n",
    "feature_col = 'temperature_2m_previous_day1 (°C)'\n",
    "target_col = 'temperature_2m (°C)'\n",
    "\n",
    "X = df_clean[[feature_col]].copy()\n",
    "y = df_clean[target_col].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb73a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b35715",
   "metadata": {},
   "source": [
    "## 8. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de26d3",
   "metadata": {},
   "source": [
    "### 8.1 Baseline Model: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec586b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LINEAR REGRESSION\")\n",
    "\n",
    "lr_param_grid = {}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LinearRegression(),\n",
    "    lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fe1ef",
   "metadata": {},
   "source": [
    "### 8.2 Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e08686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUPPORT VECTOR REGRESSOR\")\n",
    "\n",
    "svr_param_grid = {\n",
    "    \"svr__kernel\": [\"rbf\", \"poly\"],\n",
    "    \"svr__C\": [0.1, 1, 10, 50],\n",
    "    \"svr__epsilon\": [0.01, 0.1, 0.5],\n",
    "    \"svr__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "svr_grid = GridSearchCV(\n",
    "    SVR(),\n",
    "    svr_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "svr_grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee804cc",
   "metadata": {},
   "source": [
    "### 8.3 Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a270eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GRADIENT BOOSTING REGRESSOR\")\n",
    "\n",
    "gbr_param_grid = {\n",
    "    \"gbr__n_estimators\": [100, 200, 400],\n",
    "    \"gbr__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"gbr__max_depth\": [2, 3, 4],\n",
    "    \"gbr__subsample\": [0.7, 1.0]\n",
    "}\n",
    "\n",
    "gbr_grid = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    gbr_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gbr_grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a5664",
   "metadata": {},
   "source": [
    "## 9. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a889ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results_df = pd.DataFrame([\n",
    "    lr_results,\n",
    "    svr_results,\n",
    "    svr_best_results,\n",
    "    gb_results,\n",
    "    gb_best_results\n",
    "]).drop(columns=['predictions'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df)\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['Test MAE'].idxmin()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n",
    "print(f\"   Test MAE:  {results_df.loc[best_model_idx, 'Test MAE']:.4f}°C\")\n",
    "print(f\"   Test RMSE: {results_df.loc[best_model_idx, 'Test RMSE']:.4f}°C\")t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57829f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² Score comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(results_df))\n",
    "ax.bar(x_pos - 0.2, results_df['Train R²'], 0.4, label='Train R²', alpha=0.8)\n",
    "ax.bar(x_pos + 0.2, results_df['Test R²'], 0.4, label='Test R²', alpha=0.8)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('R² Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "# ax.set_ylim([0.95, 1.0])  # Zoom in to see differences\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ce580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display how much the best model improves forecast accuracy\n",
    "# compared to the original raw forecast. This compares the RMSE of the raw forecast\n",
    "# (using the forecasted temperature directly) to the RMSE of the best model\n",
    "# on the test set, and reports both the absolute and percent improvement.\n",
    "\n",
    "raw_rmse = np.sqrt(mean_squared_error(df_clean['temperature_2m (°C)'], df_clean['temperature_2m_previous_day1 (°C)']))\n",
    "best_model_rmse = results_df.loc[best_model_idx, 'Test RMSE']\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "improvement = raw_rmse - best_model_rmse\n",
    "percent_improvement = (improvement / raw_rmse) * 100\n",
    "\n",
    "print(f\"Raw forecast RMSE: {raw_rmse:.2f}°C\")\n",
    "print(f\"{best_model_name} RMSE: {best_model_rmse:.2f}°C\")\n",
    "print(f\"Absolute improvement: {improvement:.2f}°C\")\n",
    "print(f\"Percent improvement: {percent_improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac1ecc",
   "metadata": {},
   "source": [
    "## 10. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effe167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After dropping NaNs, reset index to ensure alignment\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# When plotting predictions, use a direct range for the test set\n",
    "sample_size = min(24 * 7, len(y_test))\n",
    "sample_indices = range(sample_size)\n",
    "\n",
    "# Get timestamps for test set (last len(y_test) rows after reset_index)\n",
    "test_times = df_clean['time'].iloc[-len(y_test):].values\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.plot(test_times[sample_indices], y_test.values[sample_indices],\n",
    "         label='Actual Temperature', linewidth=2, color='black', marker='o', markersize=3)\n",
    "\n",
    "plt.plot(test_times[sample_indices], lr_results['predictions'][sample_indices],\n",
    "         label='Linear Regression', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "plt.plot(test_times[sample_indices], svr_best_results['predictions'][sample_indices],\n",
    "         label='SVR (tuned)', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "plt.plot(test_times[sample_indices], gb_best_results['predictions'][sample_indices],\n",
    "         label='Gradient Boosting (tuned)', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "plt.title('Model Predictions vs Actual Temperature (First Week of Test Set)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for best models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_to_plot = [\n",
    "    ('Linear Regression', lr_results['predictions']),\n",
    "    ('SVR (tuned)', svr_best_results['predictions']),\n",
    "    ('Gradient Boosting (tuned)', gb_best_results['predictions'])\n",
    "]\n",
    "\n",
    "for idx, (name, predictions) in enumerate(models_to_plot):\n",
    "    residuals = y_test.values - predictions\n",
    "    \n",
    "    axes[idx].scatter(predictions, residuals, alpha=0.5, s=10)\n",
    "    axes[idx].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_xlabel('Predicted Temperature (°C)')\n",
    "    axes[idx].set_ylabel('Residuals (°C)')\n",
    "    axes[idx].set_title(f'{name}\\nResidual Plot', fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf82959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, predictions) in enumerate(models_to_plot):\n",
    "    errors = np.abs(y_test.values - predictions)\n",
    "    \n",
    "    axes[idx].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(errors.mean(), color='red', linestyle='--', \n",
    "                     linewidth=2, label=f'Mean: {errors.mean():.3f}°C')\n",
    "    axes[idx].set_xlabel('Absolute Error (°C)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'{name}\\nError Distribution', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc94c8f",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for best model\n",
    "if best_model_name.startswith('Gradient Boosting'):\n",
    "    best_estimator = gb_grid.best_estimator_\n",
    "    importances = best_estimator.feature_importances_\n",
    "    importance_label = 'Feature Importance'\n",
    "elif best_model_name.startswith('Linear Regression'):\n",
    "    best_estimator = lr_model\n",
    "    importances = np.abs(best_estimator.coef_)\n",
    "    importance_label = 'Absolute Coefficient'\n",
    "elif best_model_name.startswith('SVR'):\n",
    "    print(\"Feature importance is not directly available for SVR models.\")\n",
    "    importances = None\n",
    "\n",
    "if importances is not None:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': [feature_col],\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(20), feature_importance['importance'].head(20))\n",
    "    plt.yticks(range(20), feature_importance['feature'].head(20))\n",
    "    plt.xlabel(importance_label)\n",
    "    plt.title(f'Top 20 Most Important Features ({best_model_name})', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    display(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Absolute error of raw forecast vs best model on test set (auto-select best model)\n",
    "\n",
    "# Get test set indices (last len(y_test) rows after reset_index)\n",
    "test_times = df_clean['time'].iloc[-len(y_test):].values\n",
    "raw_forecast = df_clean['temperature_2m_previous_day1 (°C)'].iloc[-len(y_test):].values\n",
    "\n",
    "# Map model names to their predictions\n",
    "model_predictions = {\n",
    "    'Linear Regression': lr_results['predictions'],\n",
    "    'SVR (default)': svr_results['predictions'],\n",
    "    'SVR (tuned)': svr_best_results['predictions'],\n",
    "    'Gradient Boosting (default)': gb_results['predictions'],\n",
    "    'Gradient Boosting (tuned)': gb_best_results['predictions'],\n",
    "}\n",
    "\n",
    "# Compute absolute errors\n",
    "raw_error = np.abs(raw_forecast - y_test.values)\n",
    "best_model_error = np.abs(model_predictions[best_model_name] - y_test.values)\n",
    "\n",
    "# Compute the difference in absolute error\n",
    "error_difference = raw_error - best_model_error\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "ax.plot(test_times[sample_indices], error_difference[sample_indices],\n",
    "    color='blue', alpha=0.6, linewidth=1)\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax.set_title(f'Error Difference: Raw Forecast Error - {best_model_name} Error',\n",
    "         fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Error Difference')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight where best model improves (positive = raw worse than model)\n",
    "ax.fill_between(test_times[sample_indices], error_difference[sample_indices], 0,\n",
    "        where=error_difference[sample_indices] >= 0,\n",
    "        facecolor='green', alpha=0.2, interpolate=True)\n",
    "ax.fill_between(test_times[sample_indices], error_difference[sample_indices], 0,\n",
    "        where=error_difference[sample_indices] < 0,\n",
    "        facecolor='red', alpha=0.2, interpolate=True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b9649",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0bac8",
   "metadata": {},
   "source": [
    "After testing out linear regression, SVR, and gradient boosting for predicting the next day’s temperature using just the forecasted value, we found that the default SVR model actually worked best on our test data. The forecast itself turned out to be a strong predictor, but we noticed it often underestimated the actual temperature. By applying a machine learning model, we managed to bring the RMSE down from 2.23°C (just using the raw forecast) to 1.82°C with SVR. That’s an improvement of about 0.41°C, or roughly 18%. Overall, even with just one main feature, these simple models did a pretty good job, and it shows that correcting for systematic bias in forecasts can make a noticeable difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
